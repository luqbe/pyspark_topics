{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0c441d-2c90-4128-8db9-0a1df8560609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+---------------+----------+------------+\n|employee_id| name|salary|       email_id| join_date|date_of_join|\n+-----------+-----+------+---------------+----------+------------+\n|        101| John| 45000| bebe@gmail.com|2023-12-23|  2023-01-23|\n|        102|Kevin| 35000|  kev@gamil.com|2023-06-15|  2023-01-15|\n|        103| sara| 42000| sara@gamil.com|2022-09-24|  2022-01-24|\n|        104|Elvin| 32000|elv@outlook.com|2024-08-17|  2024-01-17|\n|        105|simba| 36000|sim@outlook.com|2025-02-15|  2025-01-15|\n+-----------+-----+------+---------------+----------+------------+\n\n+-----------+-----+------+---------------+----------+------------+-------------+\n|employee_id| name|salary|       email_id| join_date|date_of_join|double_salary|\n+-----------+-----+------+---------------+----------+------------+-------------+\n|        101| John| 45000| bebe@gmail.com|2023-12-23|  2023-01-23|        90000|\n|        102|Kevin| 35000|  kev@gamil.com|2023-06-15|  2023-01-15|        70000|\n|        103| sara| 42000| sara@gamil.com|2022-09-24|  2022-01-24|        84000|\n|        104|Elvin| 32000|elv@outlook.com|2024-08-17|  2024-01-17|        64000|\n|        105|simba| 36000|sim@outlook.com|2025-02-15|  2025-01-15|        72000|\n+-----------+-----+------+---------------+----------+------------+-------------+\n\n+-----------+-----+------+---------------+------------+-------------+\n|employee_id| name|salary|       email_id|date_of_join|double_salary|\n+-----------+-----+------+---------------+------------+-------------+\n|        101| John| 45000| bebe@gmail.com|  2023-01-23|        90000|\n|        102|Kevin| 35000|  kev@gamil.com|  2023-01-15|        70000|\n|        103| sara| 42000| sara@gamil.com|  2022-01-24|        84000|\n|        104|Elvin| 32000|elv@outlook.com|  2024-01-17|        64000|\n|        105|simba| 36000|sim@outlook.com|  2025-01-15|        72000|\n+-----------+-----+------+---------------+------------+-------------+\n\n+-----------+-------------+------+---------------+------------+-------------+\n|employee_id|employee_name|salary|       email_id|date_of_join|double_salary|\n+-----------+-------------+------+---------------+------------+-------------+\n|        101|         John| 45000| bebe@gmail.com|  2023-01-23|        90000|\n|        102|        Kevin| 35000|  kev@gamil.com|  2023-01-15|        70000|\n|        103|         sara| 42000| sara@gamil.com|  2022-01-24|        84000|\n|        104|        Elvin| 32000|elv@outlook.com|  2024-01-17|        64000|\n|        105|        simba| 36000|sim@outlook.com|  2025-01-15|        72000|\n+-----------+-------------+------+---------------+------------+-------------+\n\n+-----------+-------------+------+---------------+------------+-------------+\n|employee_id|employee_name|salary|       email_id|date_of_join|double_salary|\n+-----------+-------------+------+---------------+------------+-------------+\n|        102|        Kevin| 35000|  kev@gamil.com|  2023-01-15|        70000|\n|        104|        Elvin| 32000|elv@outlook.com|  2024-01-17|        64000|\n|        105|        simba| 36000|sim@outlook.com|  2025-01-15|        72000|\n+-----------+-------------+------+---------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pyspark\") \\\n",
    "    .getOrCreate()  \n",
    "from pyspark.sql import SparkSession    \n",
    "from pyspark.sql.types import StructType,StructField, IntegerType,StringType ,DateType \n",
    "schema=StructType([\n",
    "        StructField(\"employee_id\" ,IntegerType(),True),\n",
    "        StructField(\"name\", StringType(),True),\n",
    "        StructField(\"salary\",IntegerType(),True),\n",
    "        StructField(\"email_id\",StringType(),True),\n",
    "        StructField(\"join_date\",StringType(),True)\n",
    "])\n",
    "\n",
    "data=(\n",
    "    [101,\"John\",45000,\"bebe@gmail.com\",\"2023-12-23\"],\n",
    "    [102,\"Kevin\",35000,\"kev@gamil.com\",\"2023-06-15\"],\n",
    "    [103,\"sara\",42000,\"sara@gamil.com\",\"2022-09-24\"],\n",
    "    [104,\"Elvin\",32000,\"elv@outlook.com\",\"2024-08-17\"],\n",
    "    [105,\"simba\",36000,\"sim@outlook.com\" ,\"2025-02-15\"])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "from pyspark.sql.functions import to_date,col\n",
    "df1= df.withColumn(\"date_of_join\",to_date(\"join_date\",\"yyyy-mm-dd\")) \n",
    "\n",
    "df1.show() \n",
    "\n",
    "df2=df1.withColumn(\"double_salary\",df1[\"salary\"]*2)\n",
    "df2.show()\n",
    "df3=df2.drop(\"join_date\")\n",
    "df3.show()\n",
    "df4=df3.withColumnRenamed(\"name\",\"employee_name\")\n",
    "df4.show()\n",
    "df5=df4.filter(col(\"double_salary\") < 73000)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db302f44-5d6c-481e-a531-22e0226a9e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|employee_id|avg_salary|\n+-----------+----------+\n|        101|   45000.0|\n|        102|   35000.0|\n|        103|   42000.0|\n|        104|   32000.0|\n|        105|   36000.0|\n+-----------+----------+\n\n+-----------+----------+\n|employee_id|avg_salary|\n+-----------+----------+\n|        102|   35000.0|\n|        104|   32000.0|\n|        105|   36000.0|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,count,max,min\n",
    "df6=df4.groupby(\"employee_id\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "df6.show()\n",
    "df7=df6.filter(col(\"avg_salary\")<40000)\n",
    "df7.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5bb864-8948-4c3f-a6d5-a50362551b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------+-------------+-----------+------+----------+----------+----------+------+\n|employee_name|upper_name|lower_name|substing_name|reg_emailid|in_str|ltrim_name|rtrim_name|  rep_name|len_id|\n+-------------+----------+----------+-------------+-----------+------+----------+----------+----------+------+\n|         John|      JOHN|      john|          Joh|       bebe|     0|      John|      John|  JohnJohn|    14|\n|        Kevin|     KEVIN|     kevin|          Kev|        kev|     0|     Kevin|     Kevin|KevinKevin|    13|\n|         sara|      SARA|      sara|          sar|       sara|     0|      sara|      sara|  sarasara|    14|\n|        Elvin|     ELVIN|     elvin|          Elv|        elv|     0|     Elvin|     Elvin|ElvinElvin|    15|\n|        simba|     SIMBA|     simba|          sim|        sim|     0|     simba|     simba|simbasimba|    15|\n+-------------+----------+----------+-------------+-----------+------+----------+----------+----------+------+\n\n+----------+----------+\n|  rep_name|split_name|\n+----------+----------+\n|  JohnJohn|  JohnJohn|\n|KevinKevin|KevinKevin|\n|  sarasara|  sarasara|\n|ElvinElvin|ElvinElvin|\n|simbasimba|simbasimba|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, length, lower, upper, trim, ltrim, rtrim, concat, substring, instr, regexp_replace, regexp_extract,repeat,length,split\n",
    "df_string=df4.select(\n",
    "          col(\"employee_name\"),\n",
    "          upper(col(\"employee_name\")).alias(\"upper_name\"),\n",
    "          lower(col(\"employee_name\")).alias(\"lower_name\"),\n",
    "          substring(col(\"employee_name\"),1,3).alias(\"substing_name\"),\n",
    "          regexp_extract(col(\"email_id\"),\"([a-z]+)@\", 1).alias(\"reg_emailid\"),\n",
    "          instr(col(\"employee_id\"),\"e\").alias(\"in_str\"),\n",
    "          ltrim(col(\"employee_name\")).alias(\"ltrim_name\"),\n",
    "          rtrim(col(\"employee_name\")).alias(\"rtrim_name\"),\n",
    "          repeat(col(\"employee_name\"),2).alias(\"rep_name\"),\n",
    "          length(col(\"email_id\")).alias(\"len_id\")\n",
    "          \n",
    ")\n",
    "df_string.show()\n",
    "df_split=df_string.select(\n",
    "    col(\"rep_name\"),\n",
    "    split(col(\"rep_name\"),\"\\\\|\").getItem(0).alias(\"split_name\")\n",
    ")\n",
    "df_split.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38221379-5a4b-44a0-9def-41f4d887db79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+-----------+-----+------+---------------+----------+------------+\n|employee_id|department|   city|reporting_manger|employee_id| name|salary|       email_id| join_date|date_of_join|\n+-----------+----------+-------+----------------+-----------+-----+------+---------------+----------+------------+\n|        101|        HR|     NY|          Kritin|        101| John| 45000| bebe@gmail.com|2023-12-23|  2023-01-23|\n|        102|        IT|Kolkata|          Michel|        102|Kevin| 35000|  kev@gamil.com|2023-06-15|  2023-01-15|\n|        102|        IT|kolkata|          Michel|        102|Kevin| 35000|  kev@gamil.com|2023-06-15|  2023-01-15|\n|        103| logistics|   null|             Ram|        103| sara| 42000| sara@gamil.com|2022-09-24|  2022-01-24|\n|        105|        HR|chennai|            null|        105|simba| 36000|sim@outlook.com|2025-02-15|  2025-01-15|\n+-----------+----------+-------+----------------+-----------+-----+------+---------------+----------+------------+\n\n+---------------+-----+----------+\n|       email_id| name|department|\n+---------------+-----+----------+\n| bebe@gmail.com| John|        HR|\n|  kev@gamil.com|Kevin|        IT|\n|  kev@gamil.com|Kevin|        IT|\n| sara@gamil.com| sara| logistics|\n|elv@outlook.com|Elvin|      null|\n|sim@outlook.com|simba|        HR|\n+---------------+-----+----------+\n\n+-----+-------+----------------+\n| name|   city|reporting_manger|\n+-----+-------+----------------+\n| John|     NY|          Kritin|\n|Kevin|kolkata|          Michel|\n|Kevin|Kolkata|          Michel|\n| sara|   null|             Ram|\n|Elvin|   null|            null|\n|simba|chennai|            null|\n+-----+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder \\\n",
    "    .appName(\"null_values\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, IntegerType,StringType ,DateType \n",
    "schema=StructType([\n",
    "       StructField(\"employee_id\",IntegerType(),True),\n",
    "       StructField(\"department\",StringType(),True),\n",
    "       StructField(\"city\",StringType(),True),\n",
    "       StructField(\"reporting_manger\",StringType(),True)\n",
    "])    \n",
    "data=(\n",
    "     [101,\"HR\",\"NY\",\"Kritin\"],\n",
    "     [102,\"IT\",\"Kolkata\",\"Michel\"],\n",
    "     [103,\"logistics\",None,\"Ram\"],\n",
    "     [102,\"IT\",\"kolkata\",\"Michel\"],\n",
    "     [105,\"HR\",\"chennai\",None]\n",
    ")\n",
    "df_1 = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df_join=df_1.join(df1, df_1[\"employee_id\"]==df1[\"employee_id\"],\"inner\")\n",
    "\n",
    "df_join.show()\n",
    "df_left=df1.join(df_1, df1[\"employee_id\"]==df_1[\"employee_id\"],\"left\") \\\n",
    "            .select(\"email_id\",\"name\",\"department\")\n",
    "df_left.show()\n",
    "\n",
    "df_right=df_1.join(df1,df_1[\"employee_id\"]==df1[\"employee_id\"],\"right\")\\\n",
    "             .select(\"name\",\"city\",\"reporting_manger\")\n",
    "             \n",
    "df_right.show()             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36141d51-049d-4735-9d59-ce0a1ddefb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------------+-------+\n| name|   city|reporting_manger|row_num|\n+-----+-------+----------------+-------+\n|Elvin|   null|            null|      1|\n| John|     NY|          Kritin|      1|\n|Kevin|kolkata|          Michel|      1|\n|Kevin|Kolkata|          Michel|      2|\n| sara|   null|             Ram|      1|\n|simba|chennai|            null|      1|\n+-----+-------+----------------+-------+\n\n+-----+-------+----------------+\n| name|   city|reporting_manger|\n+-----+-------+----------------+\n|Elvin|   null|            null|\n| John|     NY|          Kritin|\n|Kevin|kolkata|          Michel|\n| sara|   null|             Ram|\n|simba|chennai|            null|\n+-----+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(col(\"reporting_manger\").desc())\n",
    "\n",
    "df_with_rownum = df_right.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "df_with_rownum.show()\n",
    "df_latest = df_with_rownum.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "df_latest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6d2675-6c55-4a9a-9aab-678467f12223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+\n|employee_id|department|   city|reporting_manger|\n+-----------+----------+-------+----------------+\n|        101|        HR|     NY|          Kritin|\n|        102|        IT|Kolkata|          Michel|\n|        102|        IT|kolkata|          Michel|\n+-----------+----------+-------+----------------+\n\n+-----------+----------+-------+----------------+\n|employee_id|department|   city|reporting_manger|\n+-----------+----------+-------+----------------+\n|        101|        HR|     NY|          Kritin|\n|        102|        IT|Kolkata|          Michel|\n|        102|        IT|kolkata|          Michel|\n+-----------+----------+-------+----------------+\n\n+-----------+----------+-------+----------------+\n|employee_id|department|   city|reporting_manger|\n+-----------+----------+-------+----------------+\n|        101|        HR|     NY|          Kritin|\n|        102|        IT|Kolkata|          Michel|\n|        103| logistics|unknown|             Ram|\n|        102|        IT|kolkata|          Michel|\n|        105|        HR|chennai|    not_assigned|\n+-----------+----------+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_drop=df_1.dropna()\n",
    "df_drop.show()\n",
    "df1_drop=df_1.dropna(subset=[\"reporting_manger\",\"city\"])\n",
    "df1_drop.show()\n",
    "df2_fill=df_1.fillna({\"city\":\"unknown\",\"reporting_manger\":\"not_assigned\"})\n",
    "df2_fill.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41072cb-071e-4eb3-af63-77a9d65ff1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|max_salary|avg_salary|\n+----------+----------+\n|     45000|   38000.0|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, round, abs\n",
    "df_agg=df1.select(max(\"salary\").alias(\"max_salary\"),\n",
    "           avg(\"salary\").alias (\"avg_salary\")\n",
    "           )\n",
    "df_agg.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67efe5a-e367-45c3-b5c1-7b90c34e435a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+----------+\n| name|sum_value|min_values|max_values|\n+-----+---------+----------+----------+\n| John|    90000|     45000|     90000|\n|Kevin|    70000|     35000|     70000|\n| sara|    84000|     42000|     84000|\n|Elvin|    64000|     32000|     64000|\n|simba|    72000|     36000|     72000|\n+-----+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,min,max\n",
    "df_group=df2.groupby(\"name\").agg(sum(\"double_salary\").alias(\"sum_value\"),\n",
    "                                 min(\"salary\").alias(\"min_values\"),\n",
    "                                 max(\"double_salary\").alias(\"max_values\")\n",
    ")\n",
    "df_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8037208a-57a4-4204-8e76-1b6dcb9e533c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+----------+\n| name|sum_value|min_values|max_values|\n+-----+---------+----------+----------+\n| John|    90000|     45000|     90000|\n| sara|    84000|     42000|     84000|\n|Elvin|    64000|     32000|     64000|\n+-----+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_filter=df_group.filter((col(\"max_values\")>75000)|(col(\"name\")==\"Elvin\")\n",
    ")\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2b71d8-593e-4d2f-9d0c-6ba2d282aa66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+-------------+\n|department|distinct_salary|first_date|   set_values|\n+----------+---------------+----------+-------------+\n|        HR|              2|2023-12-23|[simba, John]|\n| logistics|              1|2022-09-24|       [sara]|\n|        IT|              1|2023-06-15|      [Kevin]|\n+----------+---------------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list,collect_set,first,last,countDistinct\n",
    "df_grouped=df_join.groupby(\"department\").agg(\n",
    "             countDistinct(\"salary\").alias(\"distinct_salary\"),\n",
    "             first(\"join_date\").alias(\"first_date\"),\n",
    "             collect_set(\"name\").alias(\"set_values\")\n",
    ")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff37d70-f689-44eb-b053-5451648d9979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-----+\n| join_date|salary|row_num| name|\n+----------+------+-------+-----+\n|2024-08-17| 32000|      1|Elvin|\n|2023-06-15| 35000|      1|Kevin|\n|2025-02-15| 36000|      1|simba|\n|2022-09-24| 42000|      1| sara|\n|2023-12-23| 45000|      1| John|\n+----------+------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,col\n",
    "window_spec=Window.partitionBy(\"salary\").orderBy(\"join_date\")\n",
    "row_num=df1.withColumn(\"row_num\",row_number().over(window_spec))\n",
    "row_num1=row_num.select(col(\"join_date\"),\n",
    "                       col(\"salary\"),col(\"row_num\"),col(\"name\")\n",
    "                    )\n",
    "row_num1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9d553b-6cc6-4364-b24e-31097dbc00da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+\n| name|salary|rank|\n+-----+------+----+\n|Elvin| 32000|   1|\n|Kevin| 35000|   2|\n|simba| 36000|   3|\n| sara| 42000|   4|\n| John| 45000|   5|\n+-----+------+----+\n\n+-----------+----------+-------+\n|employee_id|avg_salary|   lead|\n+-----------+----------+-------+\n|        102|   35000.0|32000.0|\n|        104|   32000.0|36000.0|\n|        105|   36000.0|   null|\n+-----------+----------+-------+\n\n+-----+---------+----------+----------+-----+\n| name|sum_value|min_values|max_values|  lag|\n+-----+---------+----------+----------+-----+\n|Elvin|    64000|     32000|     64000| null|\n| John|    90000|     45000|     90000|64000|\n|Kevin|    70000|     35000|     70000|90000|\n| sara|    84000|     42000|     84000|70000|\n|simba|    72000|     36000|     72000|84000|\n+-----+---------+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank,lead,lag,col\n",
    "window_spec=Window.orderBy(\"salary\")\n",
    "df_rank=df1.withColumn(\"rank\",rank().over(window_spec))\n",
    "df_rank1=df_rank.select(col(\"name\"),col(\"salary\"),col(\"rank\"))\n",
    "df_rank1.show()\n",
    "window_spec=Window.orderBy(\"employee_id\")\n",
    "df_lead=df7.withColumn(\"lead\",lead(\"avg_salary\").over(window_spec))\n",
    "df_lead.show()\n",
    "window_spec=Window.orderBy(\"name\")\n",
    "df_lag=df_group.withColumn(\"lag\",lag(\"sum_value\").over(window_spec))\n",
    "df_lag.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}