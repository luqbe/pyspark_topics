{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c80da0b-58d7-4311-8d02-602e029cb301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|  Alice| 23|\n|  2|    Bob| 30|\n|  3|Charlie| 25|\n+---+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"InsertValues\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Insert Values\n",
    "data = [\n",
    "    (1, \"Alice\", 23),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 25)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4267002-4ef6-4ffe-9718-c83ace4f403e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------+\n|id |name   |scores         |\n+---+-------+---------------+\n|1  |Alice  |[10, 20, 30]   |\n|2  |Bob    |[5, 15]        |\n|3  |Charlie|[8, 12, 18, 22]|\n+---+-------+---------------+\n\n+-------+-----+--------------------------+\n|   name|value|array_contains(scores, 10)|\n+-------+-----+--------------------------+\n|  Alice|    3|                      true|\n|    Bob|    2|                     false|\n|Charlie|    4|                     false|\n+-------+-----+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import array, col,array_contains\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ArrayExample\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"Alice\", [10, 20, 30]),\n",
    "    (2, \"Bob\", [5, 15]),\n",
    "    (3, \"Charlie\", [8, 12, 18, 22])\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"scores\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import size,array_min,array_max,array_contains\n",
    "df1=df.select(col(\"name\"),size(\"scores\").alias(\"value\"),array_contains(\"scores\",10))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d635840-3d42-41fd-b389-4187da0f42fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n|   name|col|\n+-------+---+\n|  Alice| 10|\n|  Alice| 20|\n|  Alice| 30|\n|    Bob|  5|\n|    Bob| 15|\n|Charlie|  8|\n|Charlie| 12|\n|Charlie| 18|\n|Charlie| 22|\n+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2=df.select(col(\"name\"),explode(col(\"scores\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbe272e-ad95-4360-9aac-3215bd29fb4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------------+----------------------------------------------+\n|id |name |age|city         |json_data                                     |\n+---+-----+---+-------------+----------------------------------------------+\n|1  |Alice|25 |New York     |{\"name\":\"Alice\",\"age\":25,\"city\":\"New York\"}   |\n|2  |Bob  |30 |San Francisco|{\"name\":\"Bob\",\"age\":30,\"city\":\"San Francisco\"}|\n+---+-----+---+-------------+----------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import struct, to_json\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"JSON Example\").getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"New York\"),\n",
    "    (2, \"Bob\", 30, \"San Francisco\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"city\"])\n",
    "\n",
    "# Convert Struct to JSON String\n",
    "df_json = df.withColumn(\"json_data\", to_json(struct(\"name\", \"age\", \"city\")))\n",
    "df_json.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0e7bb2-845c-407c-ad9c-837266105d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_array_map",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}